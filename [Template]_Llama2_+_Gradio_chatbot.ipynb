{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYfudoZNOkm7II4a3dqnOF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imywong/starter_templates/blob/main/%5BTemplate%5D_Llama2_%2B_Gradio_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Initial setup"
      ],
      "metadata": {
        "id": "5gPwDPXYFqSS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Huggingface Hub library"
      ],
      "metadata": {
        "id": "DDJNau47uRYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "6tGdHgadHHzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import modules"
      ],
      "metadata": {
        "id": "bn37M_kGabDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import os"
      ],
      "metadata": {
        "id": "Wz8SF1Ezadvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply your Huggingface token\n",
        "\n",
        "Note: Follow the instructions [here](https://huggingface.co/docs/api-inference/quicktour#get-your-api-token) to get a token\n",
        "\n"
      ],
      "metadata": {
        "id": "AWY-PZ4etS8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paste in your secret key when prompted\n",
        "from getpass import getpass\n",
        "HUGGINGFACEHUB_API_TOKEN = getpass()"
      ],
      "metadata": {
        "id": "Q4a2_LKJIQ64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set token to environment variable\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN"
      ],
      "metadata": {
        "id": "Kr8ccRHC5CYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2 : Create a helper function to call the LLama2 70b chat model"
      ],
      "metadata": {
        "id": "f51YkIqmswV0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define model API url"
      ],
      "metadata": {
        "id": "sBeRsjMaIAer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "API_URL = \"https://api-inference.huggingface.co/models/meta-llama/Llama-2-70b-chat-hf\"\n",
        "headers = {\"Authorization\": f\"Bearer {HUGGINGFACEHUB_API_TOKEN}\"}"
      ],
      "metadata": {
        "id": "iAMjlf_7Cu3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the system prompt"
      ],
      "metadata": {
        "id": "LRiPU6Z52qpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"You are a helpful AI Assistant\""
      ],
      "metadata": {
        "id": "zD4Bm5RK1yCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the model parameters"
      ],
      "metadata": {
        "id": "NnO_seIdedD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_k=10 #limits the number of words that can be generated at each step to the top k most probable words. For example, if k=10, only the top 10 most probable words will be considered at each step. This method is useful when you want to generate text that is more conservative and predictable.\n",
        "top_p=0.9 #also known as nucleus sampling, is a method that limits the number of words that can be generated at each step to a cumulative probability p. For example, if p=0.9, only the words that have a cumulative probability of 0.9 or less will be considered at each step. This method is useful when you want to generate text that is more diverse and creative.\n",
        "temperature=0.7 #A higher temperature value will result in more diverse and creative text, while a lower temperature value will result in more predictable and conservative text\n",
        "max_new_tokens=500\n",
        "max_time=120\n",
        "return_full_text=False\n",
        "repetition_penalty=50.0"
      ],
      "metadata": {
        "id": "7mLX6GCnef98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the helper function"
      ],
      "metadata": {
        "id": "N453Cien2tL1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Option 1 : With memory"
      ],
      "metadata": {
        "id": "0ngIYvtYkiQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This stores the conversation history\n",
        "\n",
        "messages = f\"\"\"<s>[INST] <<SYS>>\n",
        "              {system_prompt}\n",
        "              <</SYS>>\"\"\""
      ],
      "metadata": {
        "id": "kYfWoIt1gpE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_generation(prompt, history):\n",
        "\n",
        "    global messages\n",
        "\n",
        "    messages= f\"\"\"{messages}{prompt}[/INST]\"\"\" # Add prompt to message before passing in\n",
        "\n",
        "    payload = {\n",
        "              \"inputs\": f\"{messages}\",\n",
        "                \"parameters\":{\n",
        "                    \"top_k\":top_k,\n",
        "                    \"top_p\": top_p,\n",
        "                    \"temperature\":temperature,\n",
        "                    \"max_new_tokens\":max_new_tokens,\n",
        "                    \"max_time\":max_time,\n",
        "                    \"return_full_text\":return_full_text,\n",
        "                    # \"repetition_penalty\":repetition_penaly\n",
        "                }\n",
        "            }\n",
        "    response = requests.post(API_URL, headers=headers, json=payload)\n",
        "    result_json = response.json()\n",
        "    result = result_json[0]['generated_text']\n",
        "\n",
        "    messages= f\"\"\"{messages}{result}</s><s>[INST]\"\"\" # Append latest output to message so context is available on next request\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "3hAXzEVRidhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Option 2 : Without memory"
      ],
      "metadata": {
        "id": "j2IUx0UekkyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_generation(prompt, history):\n",
        "\n",
        "    messages = f\"\"\"<s>[INST] <<SYS>>\n",
        "              {system_prompt}\n",
        "              <</SYS>>\n",
        "\n",
        "              {prompt} [/INST]\"\"\"\n",
        "\n",
        "    payload = {\n",
        "              \"inputs\": f\"{messages}\",\n",
        "                \"parameters\":{\n",
        "                    \"top_k\":top_k,\n",
        "                    \"top_p\": top_p,\n",
        "                    \"temperature\":temperature,\n",
        "                    \"max_new_tokens\":max_new_tokens,\n",
        "                    \"max_time\":max_time,\n",
        "                    \"return_full_text\":return_full_text,\n",
        "                    # \"repetition_penalty\":repetition_penaly\n",
        "                }\n",
        "            }\n",
        "    response = requests.post(API_URL, headers=headers, json=payload)\n",
        "    result_json = response.json()\n",
        "    result = result_json[0]['generated_text']\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "f5TzjBdfDaul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3 : Test the helper function"
      ],
      "metadata": {
        "id": "8kIBJvnRVQZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the question"
      ],
      "metadata": {
        "id": "0ozp4FFZIJZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is one plus one?\"  # @param {type:\"string\"}"
      ],
      "metadata": {
        "id": "pUMRARXhDcFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate a response"
      ],
      "metadata": {
        "id": "f4GYlEj-ILj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = text_generation(prompt,None)\n",
        "print(response) # Verify what was returned"
      ],
      "metadata": {
        "id": "iXhpjCvhDfrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4 : Generate a simple chat UI using Gradio\n",
        "\n",
        "A UI makes it easier to demonstrate the capability to non-coders or less technical stakeholders that might feel overwhelmed when looking at a notebook.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bIK0szD3wi7M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Gradio"
      ],
      "metadata": {
        "id": "xgOCBHu0z6Qz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "KN93ed2x2HSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "id": "sazP6LZG0xBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Gradio"
      ],
      "metadata": {
        "id": "1zOgKlTM0BCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gr.ChatInterface(text_generation).launch(share=True,debug=True)"
      ],
      "metadata": {
        "id": "I-yJT8bc4Mwl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}